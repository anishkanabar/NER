{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "08BaYWMGDLyp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "whWGX-5JDj2V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "import spacy\n",
        "from spacy.training import offsets_to_biluo_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3loAL6sKnobi"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = pd.read_json(path_or_buf='admin.jsonl', lines=True)\n",
        "cls = spacy.util.get_lang_class('en')  # 1. Get Language class, e.g. English\n",
        "nlp = cls()\n",
        "\n",
        "tags_list = []\n",
        "tokens_list = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    doc = nlp(data['text'][i])\n",
        "    doc_tokens= []\n",
        "    for d in range(len(doc)):\n",
        "        doc_tokens.append(str(doc[d]))\n",
        "    tokens_list.append(doc_tokens)\n",
        "    entities = data['label'][i]\n",
        "    tags = offsets_to_biluo_tags(doc, entities)\n",
        "    tags_list.append(tags)\n",
        "\n",
        "data['tokens'] = tokens_list\n",
        "data['ner_tags'] = tags_list\n",
        "\n",
        "data = data.drop(['id', 'label', 'text'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D6We54x36S3b"
      },
      "outputs": [],
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_percent = 0.8\n",
        "train_size = int(train_percent*len(data))\n",
        "train_df = data[:train_size]\n",
        "test_df = data[train_size:]\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(len(data)))\n",
        "print(\"TRAIN Dataset: {}\".format(len(train_df)))\n",
        "print(\"TEST Dataset: {}\".format(len(test_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kY7F4Zlu6un6"
      },
      "outputs": [],
      "source": [
        "label_list = ['B-code', 'I-code', 'L-code', 'U-code', 'B-address', 'I-address', 'L-address', 'U-address', \n",
        "                 'B-event', 'I-event', 'L-event', 'U-event', 'B-name', 'I-name', 'L-name', 'U-name', 'O', '-']\n",
        "\n",
        "label_encoding_dict = {'B-code': 0, 'I-code': 1, 'L-code': 2, 'U-code': 3, 'B-address': 4, 'I-address': 5, 'L-address': 6, \n",
        "                       'U-address': 7, 'B-event': 8, 'I-event': 9, 'L-event': 10, 'U-event': 11, 'B-name': 12, 'I-name': 13,\n",
        "                       'L-name': 14, 'U-name': 15, 'O': 16, '-': 17}\n",
        "\n",
        "task = \"ner\" \n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "batch_size = 16\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def get_all_tokens_and_ner_tags(directory):\n",
        "    return pd.concat([get_tokens_and_ner_tags(os.path.join(directory, filename)) for filename in os.listdir(directory)]).reset_index().drop('index', axis=1)\n",
        "    \n",
        "def get_tokens_and_ner_tags(filename):\n",
        "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
        "        tokens = [[x.split('\\t')[0] for x in y] for y in split_list]\n",
        "        entities = [[x.split('\\t')[1][:-1] for x in y] for y in split_list] \n",
        "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
        "  \n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ovuz5AIOnZUs"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    label_all_tokens = True\n",
        "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif label[word_idx] == '0':\n",
        "                label_ids.append(0)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "        \n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "train_tokenized_datasets = train_tokenized_datasets.remove_columns(['tokens', 'ner_tags'])\n",
        "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "test_tokenized_datasets = test_tokenized_datasets.remove_columns(['tokens', 'ner_tags'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2Rj8lPMEntYe"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=1e-5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"]}\n",
        "    \n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_datasets,\n",
        "    eval_dataset=test_tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
