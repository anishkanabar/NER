{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbZJgI9n-QPP"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install seqeval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqcgRbamBG-P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from datasets import load_metric\n",
        "import warnings\n",
        "import spacy\n",
        "from spacy.training import offsets_to_biluo_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJalafPbBLGu"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "data = pd.read_json(path_or_buf='admin.jsonl', lines=True)\n",
        "cls = spacy.util.get_lang_class('en') \n",
        "nlp = cls()\n",
        "\n",
        "tags_list = []\n",
        "tokens_list = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    doc = nlp(data['text'][i])\n",
        "    doc_tokens= []\n",
        "    for d in range(len(doc)):\n",
        "        doc_tokens.append(str(doc[d]))\n",
        "    tokens_list.append(doc_tokens)\n",
        "    entities = data['label'][i]\n",
        "    tags = offsets_to_biluo_tags(doc, entities)\n",
        "    tags_list.append(tags)\n",
        "\n",
        "data['tokens'] = tokens_list\n",
        "data['ner_tags'] = tags_list\n",
        "\n",
        "data = data.drop(['id', 'label', 'text'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o82Dqaz2BUZj"
      },
      "outputs": [],
      "source": [
        "train_percent = 0.8\n",
        "train_size = int(train_percent*len(data))\n",
        "train_df = data[:train_size]\n",
        "test_df = data[train_size:]\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(len(data)))\n",
        "print(\"TRAIN Dataset: {}\".format(len(train_df)))\n",
        "print(\"TEST Dataset: {}\".format(len(test_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMTpW9RCBZZp"
      },
      "outputs": [],
      "source": [
        "label_list = ['B-code', 'I-code', 'L-code', 'U-code', 'B-address', 'I-address', 'L-address', 'U-address', \n",
        "                 'B-event', 'I-event', 'L-event', 'U-event', 'B-name', 'I-name', 'L-name', 'U-name', 'O', '-']\n",
        "\n",
        "label_encoding_dict = {'B-code': 0, 'I-code': 1, 'L-code': 2, 'U-code': 3, 'B-address': 4, 'I-address': 5, 'L-address': 6, \n",
        "                       'U-address': 7, 'B-event': 8, 'I-event': 9, 'L-event': 10, 'U-event': 11, 'B-name': 12, 'I-name': 13,\n",
        "                       'L-name': 14, 'U-name': 15, 'O': 16, '-': 17}\n",
        "\n",
        "task = \"ner\" \n",
        "model_checkpoint = \"xlnet-base-cased\"\n",
        "batch_size = 16\n",
        "    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "\n",
        "def get_all_tokens_and_ner_tags(directory):\n",
        "    return pd.concat([get_tokens_and_ner_tags(os.path.join(directory, filename)) for filename in os.listdir(directory)]).reset_index().drop('index', axis=1)\n",
        "    \n",
        "def get_tokens_and_ner_tags(filename):\n",
        "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
        "        lines = f.readlines()\n",
        "        split_list = [list(y) for x, y in itertools.groupby(lines, lambda z: z == '\\n') if not x]\n",
        "        tokens = [[x.split('\\t')[0] for x in y] for y in split_list]\n",
        "        entities = [[x.split('\\t')[1][:-1] for x in y] for y in split_list] \n",
        "    return pd.DataFrame({'tokens': tokens, 'ner_tags': entities})\n",
        "  \n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXovufyHDlL9"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    label_all_tokens = True\n",
        "    tokenized_inputs = tokenizer(list(examples[\"tokens\"]), truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif label[word_idx] == '0':\n",
        "                label_ids.append(0)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label_encoding_dict[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(label_encoding_dict[label[word_idx]] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "        \n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "train_tokenized_datasets = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "train_tokenized_datasets = train_tokenized_datasets.remove_columns(['tokens', 'ner_tags'])\n",
        "test_tokenized_datasets = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "test_tokenized_datasets = test_tokenized_datasets.remove_columns(['tokens', 'ner_tags'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run model without pre-training"
      ],
      "metadata": {
        "id": "Whmzg1fjAzGw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfWWMFQlDBn3"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=8,\n",
        "    weight_decay=1e-5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], \n",
        "            \"address-precision\": results[\"address\"][\"precision\"], \"address-recall\": results[\"address\"][\"recall\"], \"address-f1\": results[\"address\"][\"f1\"], \"address-number\": results[\"address\"][\"number\"], \n",
        "            \"name-precision\": results[\"name\"][\"precision\"], \"name-recall\": results[\"name\"][\"recall\"], \"name-f1\": results[\"name\"][\"f1\"], \"name-number\": results[\"name\"][\"number\"],  \n",
        "            \"event-precision\": results[\"event\"][\"precision\"], \"event-recall\": results[\"event\"][\"recall\"], \"event-f1\": results[\"event\"][\"f1\"], \"event-number\": results[\"event\"][\"number\"], \n",
        "            \"code-precision\": results[\"code\"][\"precision\"], \"code-recall\": results[\"code\"][\"recall\"], \"code-f1\": results[\"code\"][\"f1\"], \"code-number\": results[\"code\"][\"number\"]}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_datasets,\n",
        "    eval_dataset=test_tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")\n",
        "\n",
        "def tokenize_and_align_labels_pretraining(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "inputs = tokenizer(dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "\n",
        "tokenized_dataset= dataset.map(\n",
        "    tokenize_and_align_labels_pretraining,\n",
        "    batched=True,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "x1dIgbXJ8N5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "8NFFNf3QpZw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "id2label = {str(i): label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "\n",
        "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label=id2label, label2id=label2id)\n"
      ],
      "metadata": {
        "id": "Gar3UW292L3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-train"
      ],
      "metadata": {
        "id": "mlmTlKxtA47f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model('./xlnet_ner_pt') #For reuse"
      ],
      "metadata": {
        "id": "DPJPrTtk2UtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run model with pre-training"
      ],
      "metadata": {
        "id": "gniuL1tIA8Wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained('xlnet_ner_pt', num_labels=len(label_list), ignore_mismatched_sizes=True)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"test-{task}\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=1e-5,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "metric = load_metric(\"seqeval\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [[label_list[p] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "    true_labels = [[label_list[l] for (p, l) in zip(prediction, label) if l != -100] for prediction, label in zip(predictions, labels)]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\"precision\": results[\"overall_precision\"], \"recall\": results[\"overall_recall\"], \"f1\": results[\"overall_f1\"], \"accuracy\": results[\"overall_accuracy\"], \n",
        "            \"address-precision\": results[\"address\"][\"precision\"], \"address-recall\": results[\"address\"][\"recall\"], \"address-f1\": results[\"address\"][\"f1\"], \"address-number\": results[\"address\"][\"number\"], \n",
        "            \"name-precision\": results[\"name\"][\"precision\"], \"name-recall\": results[\"name\"][\"recall\"], \"name-f1\": results[\"name\"][\"f1\"], \"name-number\": results[\"name\"][\"number\"],  \n",
        "            \"event-precision\": results[\"event\"][\"precision\"], \"event-recall\": results[\"event\"][\"recall\"], \"event-f1\": results[\"event\"][\"f1\"], \"event-number\": results[\"event\"][\"number\"], \n",
        "            \"code-precision\": results[\"code\"][\"precision\"], \"code-recall\": results[\"code\"][\"recall\"], \"code-f1\": results[\"code\"][\"f1\"], \"code-number\": results[\"code\"][\"number\"]}\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_tokenized_datasets,\n",
        "    eval_dataset=test_tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "yKUIB-6Z2lpc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}